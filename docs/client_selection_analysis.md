# vLLM EngineCoreClient é€‰æ‹©æœºåˆ¶è¯¦è§£

## ğŸ¯ ä½ çš„é—®é¢˜

**é—®**ï¼šæ ¹æ®æˆ‘çš„æ¨¡å‹é…ç½®ï¼ŒvLLM ç”¨äº†å“ªä¸ª Clientï¼Ÿ

**ç­”**ï¼šåŸºäºä½ çš„é…ç½®ï¼ŒvLLM ä½¿ç”¨äº† **InprocClient**ï¼ˆåœ¨è¿›ç¨‹å†…å®¢æˆ·ç«¯ï¼‰

---

## ğŸ“Š Client é€‰æ‹©é€»è¾‘

### ä»£ç ä½ç½®

```python
# æ–‡ä»¶ï¼švllm/v1/engine/core_client.py

@staticmethod
def make_client(
    multiprocess_mode: bool,    # â† å…³é”®å‚æ•° 1
    asyncio_mode: bool,         # â† å…³é”®å‚æ•° 2
    vllm_config: VllmConfig,
    executor_class: type[Executor],
    log_stats: bool,
) -> "EngineCoreClient":
    logger.info("=== EngineCoreClient.make_client called ===")
    logger.info(f"Multiprocess mode: {multiprocess_mode}")
    logger.info(f"Asyncio mode: {asyncio_mode}")
    logger.info(f"Executor class: {executor_class.__name__}")
    
    # å†³ç­–æ ‘
    if multiprocess_mode and asyncio_mode:
        # åœºæ™¯ 1ï¼šå¤šè¿›ç¨‹ + å¼‚æ­¥
        logger.info("Creating AsyncMPClient...")
        return EngineCoreClient.make_async_mp_client(...)
    
    if multiprocess_mode and not asyncio_mode:
        # åœºæ™¯ 2ï¼šå¤šè¿›ç¨‹ + åŒæ­¥
        logger.info("Creating SyncMPClient...")
        return SyncMPClient(...)
    
    # åœºæ™¯ 3ï¼šå•è¿›ç¨‹ï¼ˆä½ çš„æƒ…å†µï¼‰
    logger.info("Creating InprocClient...")
    return InprocClient(...)
```

---

## ğŸ” ä½ çš„é…ç½®åˆ†æ

### ä½ çš„æ¨ç†ä»£ç 

```python
llm = LLM(
    model="Qwen/Qwen2.5-1.5B-Instruct",
    gpu_memory_utilization=0.7,
    max_model_len=1024,
    dtype="float16"
)

outputs = llm.generate(prompts, sampling_params)
```

### è°ƒç”¨é“¾è¿½è¸ª

```
1. vllm/entrypoints/llm.py: LLM.__init__()
   â†“
2. vllm/v1/engine/llm_engine.py: LLMEngine.from_engine_args()
   â†“
   engine_args = EngineArgs(
       model="Qwen/Qwen2.5-1.5B-Instruct",
       ...
   )
   â†“
3. vllm/v1/engine/llm_engine.py: LLMEngine.__init__()
   â†“
   self.engine_core = EngineCoreClient.make_client(
       multiprocess_mode=False,   # â† é»˜è®¤ Falseï¼ˆå•è¿›ç¨‹ï¼‰
       asyncio_mode=False,        # â† LLM æ˜¯åŒæ­¥çš„ï¼Œæ‰€ä»¥ False
       vllm_config=vllm_config,
       executor_class=GPUExecutor,
       log_stats=True,
   )
   â†“
4. vllm/v1/engine/core_client.py: EngineCoreClient.make_client()
   â†“
   å› ä¸º multiprocess_mode=False ä¸” asyncio_mode=False
   â†’ è¿”å› InprocClient(...)
   â†“
5. vllm/v1/engine/core_client.py: InprocClient.__init__()
   â†“
   self.engine_core = EngineCore(...)  # â† åœ¨å½“å‰è¿›ç¨‹åˆ›å»º EngineCore
   â†“
6. vllm/v1/engine/core.py: EngineCore.__init__()
   â†“
   # åŠ è½½æ¨¡å‹åˆ°å½“å‰è¿›ç¨‹
   self.model_executor = GPUExecutor(...)
   self.model_executor.initialize_model(...)
```

---

## ğŸ“ ä¸‰ç§ Client å¯¹æ¯”

### 1. InprocClientï¼ˆä½ ä½¿ç”¨çš„è¿™ä¸ªï¼‰

**ç‰¹ç‚¹**ï¼š
- âœ… åœ¨å½“å‰è¿›ç¨‹ä¸­è¿è¡Œ EngineCore
- âœ… åŒæ­¥è°ƒç”¨ï¼Œæ²¡æœ‰å¤šè¿›ç¨‹é€šä¿¡å¼€é”€
- âœ… ç®€å•ç›´æ¥ï¼Œè°ƒè¯•æ–¹ä¾¿
- âœ… æ¨¡å‹åŠ è½½åœ¨å½“å‰è¿›ç¨‹ä¸­

**é€‚ç”¨åœºæ™¯**ï¼š
- å• GPU æ¨ç†
- åŒæ­¥ APIï¼ˆ`LLM.generate()`ï¼‰
- ä¸éœ€è¦å¼‚æ­¥å¹¶å‘

**ä»£ç ç»“æ„**ï¼š
```python
class InprocClient(EngineCoreClient):
    def __init__(self, ...):
        # ç›´æ¥åœ¨å½“å‰è¿›ç¨‹åˆ›å»º EngineCore
        self.engine_core = EngineCore(...)
    
    def get_output(self):
        # ç›´æ¥è°ƒç”¨ EngineCore.step_fn()
        outputs, model_executed = self.engine_core.step_fn()
        return outputs
    
    def add_request(self, request):
        # ç›´æ¥è°ƒç”¨ EngineCore.add_request()
        self.engine_core.add_request(request)
```

**æ‰§è¡Œæµç¨‹**ï¼š
```
ä½ çš„ä»£ç ï¼šllm.generate(prompts)
    â†“ (åŒä¸€è¿›ç¨‹)
LLMEngine.step()
    â†“ (åŒä¸€è¿›ç¨‹)
InprocClient.get_output()
    â†“ (åŒä¸€è¿›ç¨‹)
EngineCore.step_fn()
    â†“ (åŒä¸€è¿›ç¨‹)
GPUExecutor.execute_model()
    â†“ (åŒä¸€è¿›ç¨‹)
Model.forward()  â† GPU è®¡ç®—
```

---

### 2. SyncMPClientï¼ˆå¤šè¿›ç¨‹åŒæ­¥ï¼‰

**ç‰¹ç‚¹**ï¼š
- ğŸ”„ EngineCore åœ¨åå°è¿›ç¨‹ä¸­è¿è¡Œ
- ğŸ”„ é€šè¿‡ ZMQ é€šä¿¡ï¼ˆè¿›ç¨‹é—´ï¼‰
- ğŸ”„ åŒæ­¥ APIï¼Œä½† EngineCore ç‹¬ç«‹è¿è¡Œ
- ğŸ”„ æ¨¡å‹åŠ è½½åœ¨åå°è¿›ç¨‹ä¸­

**é€‚ç”¨åœºæ™¯**ï¼š
- å¤š GPU æ¨ç†ï¼ˆéœ€è¦ç‹¬ç«‹è¿›ç¨‹ï¼‰
- åŒæ­¥ APIï¼ˆ`LLM.generate()`ï¼‰
- éœ€è¦éš”ç¦» EngineCore

**å¯ç”¨æ–¹å¼**ï¼š
```python
# è®¾ç½®ç¯å¢ƒå˜é‡
import os
os.environ['VLLM_ENABLE_V1_MULTIPROCESSING'] = '1'

llm = LLM(
    model="Qwen/Qwen2.5-1.5B-Instruct",
    ...
)
# â†’ ä¼šä½¿ç”¨ SyncMPClient
```

**ä»£ç ç»“æ„**ï¼š
```python
class SyncMPClient(MPClient):
    def __init__(self, ...):
        # EngineCore åœ¨åå°è¿›ç¨‹ä¸­
        # é€šè¿‡ ZMQ socket é€šä¿¡
        self.input_socket = zmq.Socket(...)
        self.output_socket = zmq.Socket(...)
    
    def get_output(self):
        # ä» ZMQ socket æ¥æ”¶è¾“å‡º
        outputs = self.outputs_queue.get()
        return outputs
    
    def add_request(self, request):
        # é€šè¿‡ ZMQ socket å‘é€è¯·æ±‚
        self.input_socket.send(request)
```

**æ‰§è¡Œæµç¨‹**ï¼š
```
ä½ çš„ä»£ç ï¼šllm.generate(prompts)
    â†“ (è¿›ç¨‹ A)
LLMEngine.step()
    â†“ (è¿›ç¨‹ A)
SyncMPClient.get_output()
    â†“ (è¿›ç¨‹ A â†’ è¿›ç¨‹ Bï¼Œé€šè¿‡ ZMQ)
EngineCore.step_fn()  [åå°è¿›ç¨‹ B]
    â†“ (è¿›ç¨‹ B)
GPUExecutor.execute_model()
    â†“ (è¿›ç¨‹ B)
Model.forward()  â† GPU è®¡ç®—
```

---

### 3. AsyncMPClientï¼ˆå¤šè¿›ç¨‹å¼‚æ­¥ï¼‰

**ç‰¹ç‚¹**ï¼š
- ğŸ”„ EngineCore åœ¨åå°è¿›ç¨‹ä¸­è¿è¡Œ
- âš¡ å¼‚æ­¥ APIï¼ˆ`async/await`ï¼‰
- ğŸ”„ é€šè¿‡ ZMQ å¼‚æ­¥é€šä¿¡
- ğŸ”„ æ”¯æŒå¹¶å‘è¯·æ±‚

**é€‚ç”¨åœºæ™¯**ï¼š
- å¼‚æ­¥ APIï¼ˆ`AsyncLLM.generate()`ï¼‰
- éœ€è¦é«˜å¹¶å‘
- åœ¨çº¿æ¨ç†æœåŠ¡

**å¯ç”¨æ–¹å¼**ï¼š
```python
from vllm import AsyncLLM

async def main():
    llm = AsyncLLM(
        model="Qwen/Qwen2.5-1.5B-Instruct",
        ...
    )
    # â†’ ä¼šä½¿ç”¨ AsyncMPClient
    
    outputs = await llm.generate(prompts)
```

**ä»£ç ç»“æ„**ï¼š
```python
class AsyncMPClient(MPClient):
    def __init__(self, ...):
        # ä½¿ç”¨ asyncio + ZMQ
        self.ctx = zmq.asyncio.Context()
        self.input_socket = zmq.asyncio.Socket(...)
        self.outputs_queue = asyncio.Queue()
    
    async def get_output_async(self):
        # å¼‚æ­¥æ¥æ”¶è¾“å‡º
        outputs = await self.outputs_queue.get()
        return outputs
    
    async def add_request_async(self, request):
        # å¼‚æ­¥å‘é€è¯·æ±‚
        await self.input_socket.send(request)
```

---

## ğŸ¯ åˆ¤æ–­ä½ ç”¨äº†å“ªä¸ª Client

### æ–¹æ³• 1ï¼šè¿è¡Œæ—¶æ—¥å¿—ï¼ˆæœ€ç›´æ¥ï¼‰

æˆ‘å·²ç»åœ¨ `core_client.py` ä¸­æ·»åŠ äº†æ—¥å¿—ã€‚è¿è¡Œä½ çš„ä»£ç æ—¶ä¼šçœ‹åˆ°ï¼š

```bash
python your_script.py

# è¾“å‡ºï¼š
=== EngineCoreClient.make_client called ===
Multiprocess mode: False          â† å…³é”®ï¼
Asyncio mode: False               â† å…³é”®ï¼
Executor class: GPUExecutor
Model: Qwen/Qwen2.5-1.5B-Instruct

Creating InprocClient...          â† ä½ ç”¨çš„æ˜¯è¿™ä¸ªï¼

================================================================================
ğŸ”¹ [InprocClient.__init__] åˆ›å»º InprocClient
================================================================================
ç‰¹ç‚¹ï¼š
  â€¢ åœ¨å½“å‰è¿›ç¨‹ä¸­è¿è¡Œ EngineCore
  â€¢ åŒæ­¥è°ƒç”¨ï¼Œæ²¡æœ‰å¤šè¿›ç¨‹
  â€¢ æ¨¡å‹åŠ è½½åœ¨å½“å‰è¿›ç¨‹ä¸­

å¼€å§‹åˆå§‹åŒ– EngineCore...
  Step 1: ä¸‹è½½æ¨¡å‹ï¼ˆå¦‚æœéœ€è¦ï¼‰
  Step 2: åŠ è½½æ¨¡å‹æƒé‡åˆ° CPU å†…å­˜
  Step 3: ä¼ è¾“æƒé‡åˆ° GPU æ˜¾å­˜
âœ… EngineCore åˆå§‹åŒ–å®Œæˆï¼ˆæ¨¡å‹å·²åŠ è½½ï¼‰
================================================================================
```

### æ–¹æ³• 2ï¼šæ£€æŸ¥ä»£ç é€»è¾‘

```python
# ä½ çš„é…ç½®
llm = LLM(model="Qwen/Qwen2.5-1.5B-Instruct", ...)

# åˆ¤æ–­é€»è¾‘ï¼š
# 1. æ˜¯åŒæ­¥ APIï¼ˆLLMï¼Œä¸æ˜¯ AsyncLLMï¼‰
#    â†’ asyncio_mode = False

# 2. æ²¡æœ‰è®¾ç½®å¤šè¿›ç¨‹ç¯å¢ƒå˜é‡
#    â†’ multiprocess_mode = False

# 3. æ ¹æ® make_client() çš„é€»è¾‘ï¼š
#    if multiprocess_mode and asyncio_mode:
#        â†’ AsyncMPClient  # âŒ ä¸æ»¡è¶³
#    if multiprocess_mode and not asyncio_mode:
#        â†’ SyncMPClient   # âŒ ä¸æ»¡è¶³
#    â†’ InprocClient       # âœ… ä½ çš„æƒ…å†µï¼
```

### æ–¹æ³• 3ï¼šè¿è¡Œæ—¶æ£€æŸ¥

```python
# åœ¨ä½ çš„ä»£ç ä¸­æ·»åŠ 
llm = LLM(model="Qwen/Qwen2.5-1.5B-Instruct", ...)

# æ£€æŸ¥ client ç±»å‹
client_type = type(llm.llm_engine.engine_core).__name__
print(f"Using client: {client_type}")

# è¾“å‡ºï¼šUsing client: InprocClient
```

---

## ğŸ“‹ æ€»ç»“

### ä½ çš„é…ç½®

```python
llm = LLM(
    model="Qwen/Qwen2.5-1.5B-Instruct",
    gpu_memory_utilization=0.7,
    max_model_len=1024,
    dtype="float16"
)
```

### ä½¿ç”¨çš„ Client

**InprocClient** âœ…

### åŸå› 

| æ¡ä»¶ | ä½ çš„æƒ…å†µ | ç»“æœ |
|------|---------|------|
| `multiprocess_mode` | Falseï¼ˆé»˜è®¤ï¼‰ | å•è¿›ç¨‹ |
| `asyncio_mode` | Falseï¼ˆLLM æ˜¯åŒæ­¥çš„ï¼‰ | åŒæ­¥ API |
| **Client é€‰æ‹©** | â†’ | **InprocClient** |

### æ‰§è¡Œæµç¨‹

```
ä½ çš„ Python è¿›ç¨‹ï¼ˆå•è¿›ç¨‹ï¼‰
â”œâ”€â”€ LLMEngine
â”‚   â””â”€â”€ InprocClient
â”‚       â””â”€â”€ EngineCoreï¼ˆåœ¨åŒä¸€è¿›ç¨‹ï¼‰
â”‚           â””â”€â”€ GPUExecutor
â”‚               â””â”€â”€ GPUWorker
â”‚                   â””â”€â”€ ModelRunner
â”‚                       â””â”€â”€ Model (Qwen2ForCausalLM)
â”‚                           â””â”€â”€ GPU Kernels
â”‚                               â”œâ”€â”€ RMSNorm
â”‚                               â”œâ”€â”€ PagedAttention
â”‚                               â”œâ”€â”€ Rotary Embedding
â”‚                               â””â”€â”€ SiLU
```

### å¦‚ä½•åˆ‡æ¢åˆ°å…¶ä»– Clientï¼Ÿ

#### åˆ‡æ¢åˆ° SyncMPClient

```python
import os
os.environ['VLLM_ENABLE_V1_MULTIPROCESSING'] = '1'

llm = LLM(model="Qwen/Qwen2.5-1.5B-Instruct", ...)
# â†’ ä½¿ç”¨ SyncMPClient
```

#### åˆ‡æ¢åˆ° AsyncMPClient

```python
from vllm import AsyncLLM

async def main():
    llm = AsyncLLM(model="Qwen/Qwen2.5-1.5B-Instruct", ...)
    # â†’ ä½¿ç”¨ AsyncMPClient
    outputs = await llm.generate(prompts)
```

---

## ğŸ”— ç›¸å…³æ–‡ä»¶

- **Client é€‰æ‹©é€»è¾‘**ï¼š`vllm/v1/engine/core_client.py:make_client()`
- **InprocClient å®ç°**ï¼š`vllm/v1/engine/core_client.py:InprocClient`
- **EngineCore**ï¼š`vllm/v1/engine/core.py:EngineCore`
- **æ¨ç†æ‰§è¡Œ**ï¼š`vllm/v1/engine/core.py:EngineCore.step_fn()`

ç°åœ¨è¿è¡Œä½ çš„ä»£ç ï¼Œä½ ä¼šåœ¨æ—¥å¿—ä¸­æ¸…æ¥šåœ°çœ‹åˆ°ä½¿ç”¨äº†å“ªä¸ª Clientï¼ğŸ¯
