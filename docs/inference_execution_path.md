# vLLM V1 æ¨ç†æ‰§è¡Œè·¯å¾„åˆ†æ

## ğŸ” ä½ çš„é—®é¢˜ï¼šåœ¨ `LLMEngine.step()` æ‰¾ä¸åˆ°æ¨ç†ä»£ç 

ä½ çœ‹åˆ°çš„ä»£ç ï¼š
```python
def step(self) -> list[RequestOutput | PoolingRequestOutput]:
    if self.should_execute_dummy_batch:
        self.should_execute_dummy_batch = False
        self.engine_core.execute_dummy_batch()
        return []
    
    # 1) Get EngineCoreOutput from the EngineCore.
    outputs = self.engine_core.get_output()  # â† æ¨ç†åœ¨è¿™é‡Œï¼
    
    # 2) Process EngineCoreOutputs.
    processed_outputs = self.output_processor.process_outputs(...)
    
    # 3) Abort any reqs that finished
    self.engine_core.abort_requests(...)
    
    return processed_outputs.request_outputs
```

**å…³é”®ç†è§£**ï¼šæ¨ç†ä»£ç ä¸åœ¨ `llm_engine.py`ï¼Œè€Œåœ¨ `engine_core` ä¸­ï¼

---

## ğŸ“Š å®Œæ•´çš„è°ƒç”¨é“¾

```
ä½ çš„ä»£ç ï¼šllm.generate(prompts)
    â†“
vllm/entrypoints/llm.py: LLM.generate()
    â†“
    while engine.has_unfinished_requests():
        outputs = engine.step()  â† ä½ çœ‹åˆ°çš„è¿™ä¸ª
    â†“
vllm/v1/engine/llm_engine.py: LLMEngine.step()
    â†“
    outputs = self.engine_core.get_output()  â† å…³é”®ï¼
    â†“
vllm/v1/engine/core_client.py: EngineCoreClient.get_output()
    â†“
vllm/v1/engine/core.py: EngineCore.step()  â† çœŸæ­£çš„æ¨ç†åœ¨è¿™é‡Œï¼
    â†“
    self._schedule()           # è°ƒåº¦è¯·æ±‚
    self._execute_model()      # ğŸ”¥ æ‰§è¡Œæ¨¡å‹ï¼ˆè°ƒç”¨ç®—å­ï¼‰
    â†“
vllm/v1/executor/gpu_executor.py: GPUExecutor.execute_model()
    â†“
vllm/v1/worker/gpu_worker.py: GPUWorker.execute_model()
    â†“
vllm/v1/worker/gpu_model_runner.py: GPUModelRunner.execute_model()
    â†“
    output = self.model(...)  â† è°ƒç”¨ Transformer æ¨¡å‹
    â†“
vllm/model_executor/models/qwen2.py: Qwen2ForCausalLM.forward()
    â†“
    for layer in self.layers:
        hidden_states = layer(hidden_states, ...)  â† é€å±‚è®¡ç®—
    â†“
vllm/model_executor/models/qwen2.py: Qwen2DecoderLayer.forward()
    â†“
    # RMSNorm
    hidden_states = self.input_layernorm(hidden_states)
    # Attention (è°ƒç”¨ PagedAttention)
    hidden_states = self.self_attn(hidden_states, kv_cache, ...)
    # MLP
    hidden_states = self.mlp(hidden_states)
    â†“
vllm/model_executor/layers/attention.py: Attention.forward()
    â†“
vllm/attention/backends/flash_attn.py: FlashAttentionImpl.forward()
    â†“
    torch.ops.vllm.paged_attention_v2(...)  â† ğŸ”¥ è°ƒç”¨ CUDA kernel
    â†“
csrc/attention/paged_attention_v2.cu: paged_attention_v2()
    â†“
    paged_attention_v2_kernel<<<>>>()        â† GPU è®¡ç®—
    paged_attention_v2_reduce_kernel<<<>>>() â† GPU å½’çº¦
```

---

## ğŸ¯ å…³é”®æ–‡ä»¶ä½ç½®

### 1. EngineCore - çœŸæ­£çš„æ¨ç†é€»è¾‘

```python
# æ–‡ä»¶ï¼švllm/v1/engine/core.py

class EngineCore:
    """æ ¸å¿ƒæ¨ç†å¼•æ“"""
    
    def step(self) -> EngineCoreOutput:
        """å•æ­¥æ¨ç†ï¼ˆè¿™é‡Œæ‰æ˜¯æ¨ç†ä»£ç ï¼ï¼‰"""
        
        # ğŸ” è°ƒåº¦ï¼šé€‰æ‹©è¦æ‰§è¡Œçš„è¯·æ±‚
        scheduler_output = self._schedule()
        
        # ğŸ”¥ æ‰§è¡Œï¼šè°ƒç”¨æ¨¡å‹å‰å‘ä¼ æ’­
        model_output = self._execute_model(scheduler_output)
        
        # ğŸ“¤ å¤„ç†è¾“å‡º
        return self._process_model_output(model_output)
    
    def _schedule(self) -> SchedulerOutput:
        """è°ƒåº¦å™¨ï¼šå†³å®šå“ªäº›è¯·æ±‚æ‰§è¡Œ"""
        # é€‰æ‹© batch
        # åˆ†é… KV cache blocks
        # æ›´æ–°è¯·æ±‚çŠ¶æ€
        pass
    
    def _execute_model(self, scheduler_output) -> ModelOutput:
        """ğŸ”¥ æ‰§è¡Œæ¨¡å‹ï¼ˆè°ƒç”¨æ‰€æœ‰ç®—å­ï¼‰"""
        
        # å‡†å¤‡è¾“å…¥
        model_input = self._prepare_model_input(scheduler_output)
        
        # ğŸ”¥ è°ƒç”¨ Executor
        output = self.model_executor.execute_model(
            execute_model_req=model_input
        )
        
        return output
```

**ä½ç½®**ï¼š`vllm/v1/engine/core.py`

### 2. ModelExecutor - æ‰§è¡Œæ¨¡å‹

```python
# æ–‡ä»¶ï¼švllm/v1/executor/gpu_executor.py

class GPUExecutor:
    def execute_model(self, execute_model_req):
        """æ‰§è¡Œæ¨¡å‹å‰å‘ä¼ æ’­"""
        
        # ğŸ”¥ è°ƒç”¨ Worker
        output = self.driver_worker.execute_model(
            execute_model_req=execute_model_req
        )
        
        return output
```

### 3. GPUWorker - å®é™…æ‰§è¡Œ

```python
# æ–‡ä»¶ï¼švllm/v1/worker/gpu_worker.py

class GPUWorker:
    def execute_model(self, execute_model_req):
        """åœ¨ GPU ä¸Šæ‰§è¡Œæ¨¡å‹"""
        
        # ğŸ”¥ è°ƒç”¨ ModelRunner
        output = self.model_runner.execute_model(
            model_input=execute_model_req.model_input,
            kv_caches=self.kv_caches,
        )
        
        return output
```

### 4. ModelRunner - è°ƒç”¨æ¨¡å‹

```python
# æ–‡ä»¶ï¼švllm/v1/worker/gpu_model_runner.py

class GPUModelRunner:
    def execute_model(self, model_input, kv_caches):
        """æ‰§è¡Œæ¨¡å‹å‰å‘ä¼ æ’­"""
        
        # å‡†å¤‡è¾“å…¥å¼ é‡
        input_ids = model_input.input_ids
        positions = model_input.positions
        
        # ğŸ”¥ è°ƒç”¨æ¨¡å‹
        hidden_states = self.model(
            input_ids=input_ids,
            positions=positions,
            kv_caches=kv_caches,
            attn_metadata=model_input.attn_metadata,
        )
        
        # Logits è®¡ç®—
        logits = self.model.compute_logits(hidden_states, ...)
        
        return logits
```

### 5. Model - Transformer å±‚

```python
# æ–‡ä»¶ï¼švllm/model_executor/models/qwen2.py

class Qwen2ForCausalLM(nn.Module):
    def forward(
        self,
        input_ids: torch.Tensor,
        positions: torch.Tensor,
        kv_caches: list[torch.Tensor],
        attn_metadata: AttentionMetadata,
    ):
        # Embedding
        hidden_states = self.embed_tokens(input_ids)
        
        # ğŸ”¥ é€å±‚è®¡ç®—
        for i, layer in enumerate(self.layers):
            hidden_states = layer(
                positions=positions,
                hidden_states=hidden_states,
                kv_cache=kv_caches[i],
                attn_metadata=attn_metadata,
            )
        
        # Final norm
        hidden_states = self.norm(hidden_states)
        
        return hidden_states
```

### 6. DecoderLayer - å•å±‚è®¡ç®—

```python
# æ–‡ä»¶ï¼švllm/model_executor/models/qwen2.py

class Qwen2DecoderLayer(nn.Module):
    def forward(self, positions, hidden_states, kv_cache, attn_metadata):
        # ğŸ”¹ Pre-Attention RMSNorm
        residual = hidden_states
        hidden_states = self.input_layernorm(hidden_states)
        
        # ğŸ”¥ Self-Attentionï¼ˆè°ƒç”¨ PagedAttentionï¼‰
        hidden_states = self.self_attn(
            positions=positions,
            hidden_states=hidden_states,
            kv_cache=kv_cache,
            attn_metadata=attn_metadata,
        )
        hidden_states = residual + hidden_states
        
        # ğŸ”¹ Post-Attention RMSNorm
        residual = hidden_states
        hidden_states = self.post_attention_layernorm(hidden_states)
        
        # ğŸ”¹ MLP
        hidden_states = self.mlp(hidden_states)
        hidden_states = residual + hidden_states
        
        return hidden_states
```

---

## ğŸ”¥ ç®—å­è°ƒç”¨çš„å…·ä½“ä½ç½®

### 1. RMSNorm ç®—å­

```python
# æ–‡ä»¶ï¼švllm/model_executor/layers/layernorm.py

class RMSNorm(nn.Module):
    def forward(self, x, residual=None):
        if residual is not None:
            # ğŸ”¥ è°ƒç”¨ CUDA kernel
            x = torch.ops.vllm.fused_add_rms_norm(
                x, residual, self.weight, self.variance_epsilon
            )
            # â†’ csrc/ops/layernorm.cu: fused_add_rms_norm_kernel<<<>>>
        else:
            # PyTorch åŸç”Ÿå®ç°
            pass
        return x
```

**CUDA æ–‡ä»¶**ï¼š`csrc/ops/layernorm.cu`

### 2. Rotary Embedding ç®—å­

```python
# æ–‡ä»¶ï¼švllm/model_executor/layers/rotary_embedding.py

class RotaryEmbedding(nn.Module):
    def forward(self, positions, query, key):
        # ğŸ”¥ è°ƒç”¨ CUDA kernel
        torch.ops.vllm.rotary_embedding(
            positions, query, key, self.head_size, ...
        )
        # â†’ csrc/ops/rotary_embedding.cu: rotary_embedding_kernel<<<>>>
        return query, key
```

**CUDA æ–‡ä»¶**ï¼š`csrc/ops/rotary_embedding.cu`

### 3. PagedAttention ç®—å­

```python
# æ–‡ä»¶ï¼švllm/attention/ops/paged_attn.py

def paged_attention_v2(...):
    """PagedAttention V2 ç®—å­"""
    
    # ğŸ”¥ è°ƒç”¨ CUDA kernels
    torch.ops.vllm.paged_attention_v2(
        out, exp_sums, max_logits, tmp_out,
        query, key_cache, value_cache,
        num_kv_heads, scale, block_tables, seq_lens,
        ...
    )
    # â†’ csrc/attention/paged_attention_v2.cu:
    #     - paged_attention_v2_kernel<<<>>>
    #     - paged_attention_v2_reduce_kernel<<<>>>
```

**CUDA æ–‡ä»¶**ï¼š`csrc/attention/paged_attention_v2.cu`

### 4. SiLU æ¿€æ´»å‡½æ•°ç®—å­

```python
# æ–‡ä»¶ï¼švllm/model_executor/layers/activation.py

class SiluAndMul(nn.Module):
    def forward(self, x):
        # ğŸ”¥ è°ƒç”¨ CUDA kernel
        torch.ops.vllm.silu_and_mul(out, x)
        # â†’ csrc/ops/activation.cu: silu_and_mul_kernel<<<>>>
        return out
```

**CUDA æ–‡ä»¶**ï¼š`csrc/ops/activation.cu`

---

## ğŸ“ å¦‚ä½•è¿½è¸ªæ¨ç†è·¯å¾„ï¼Ÿ

### æ–¹æ³• 1ï¼šæ·»åŠ æ—¥å¿—åˆ°å…³é”®æ–‡ä»¶

æˆ‘å·²ç»åœ¨ `llm_engine.py` ä¸­æ·»åŠ äº†æ—¥å¿—ã€‚ç°åœ¨ä½ éœ€è¦åœ¨å…¶ä»–å…³é”®æ–‡ä»¶ä¸­æ·»åŠ ï¼š

#### åœ¨ EngineCore ä¸­æ·»åŠ æ—¥å¿—

```python
# æ–‡ä»¶ï¼švllm/v1/engine/core.py

class EngineCore:
    def step(self):
        logger.info("ğŸ”¹ [EngineCore.step] å¼€å§‹")
        
        # è°ƒåº¦
        logger.info("   â†’ è°ƒåº¦è¯·æ±‚...")
        scheduler_output = self._schedule()
        logger.info("   â†’ é€‰ä¸­ %d ä¸ªè¯·æ±‚", len(scheduler_output.scheduled_requests))
        
        # æ‰§è¡Œæ¨¡å‹
        logger.info("   â†’ ğŸ”¥ æ‰§è¡Œæ¨¡å‹...")
        model_output = self._execute_model(scheduler_output)
        logger.info("   â†’ âœ… æ¨¡å‹æ‰§è¡Œå®Œæˆ")
        
        return self._process_model_output(model_output)
```

#### åœ¨ ModelRunner ä¸­æ·»åŠ æ—¥å¿—

```python
# æ–‡ä»¶ï¼švllm/v1/worker/gpu_model_runner.py

class GPUModelRunner:
    def execute_model(self, model_input, kv_caches):
        logger.info("ğŸ”¥ [ModelRunner] æ‰§è¡Œæ¨¡å‹å‰å‘ä¼ æ’­")
        logger.info("   â†’ input_ids shape: %s", model_input.input_ids.shape)
        
        # è°ƒç”¨æ¨¡å‹
        hidden_states = self.model(
            input_ids=model_input.input_ids,
            positions=model_input.positions,
            kv_caches=kv_caches,
            attn_metadata=model_input.attn_metadata,
        )
        
        logger.info("   â†’ âœ… hidden_states shape: %s", hidden_states.shape)
        return logits
```

### æ–¹æ³• 2ï¼šä½¿ç”¨æˆ‘åˆ›å»ºçš„è¿½è¸ªè„šæœ¬

è¿è¡Œï¼š
```bash
python test_simple_trace.py
```

è¿™ä¼šè‡ªåŠ¨è¿½è¸ªæ‰€æœ‰æ¨¡å—çš„è°ƒç”¨ã€‚

---

## ğŸ¯ æ€»ç»“

### æ¨ç†ä»£ç çš„ä½ç½®

| å±‚æ¬¡ | æ–‡ä»¶ | èŒè´£ |
|------|------|------|
| **1. å…¥å£** | `vllm/entrypoints/llm.py` | ç”¨æˆ·è°ƒç”¨ `llm.generate()` |
| **2. å¼•æ“** | `vllm/v1/engine/llm_engine.py` | å¾ªç¯è°ƒç”¨ `step()` |
| **3. æ ¸å¿ƒ** | `vllm/v1/engine/core.py` | **çœŸæ­£çš„æ¨ç†é€»è¾‘ï¼ˆè°ƒåº¦+æ‰§è¡Œï¼‰** |
| **4. æ‰§è¡Œå™¨** | `vllm/v1/executor/gpu_executor.py` | åˆ†å‘ä»»åŠ¡åˆ° Worker |
| **5. Worker** | `vllm/v1/worker/gpu_worker.py` | GPU ä¸Šæ‰§è¡Œ |
| **6. Runner** | `vllm/v1/worker/gpu_model_runner.py` | å‡†å¤‡è¾“å…¥ï¼Œè°ƒç”¨æ¨¡å‹ |
| **7. æ¨¡å‹** | `vllm/model_executor/models/qwen2.py` | Transformer å±‚ |
| **8. Layers** | `vllm/model_executor/layers/` | å„ç§ç®—å­ï¼ˆRMSNormã€Attentionã€MLPï¼‰|
| **9. CUDA** | `csrc/attention/`, `csrc/ops/` | **GPU Kernels** |

### å…³é”®ç†è§£

1. **`llm_engine.py` åªæ˜¯å¤–å£³**
   - çœŸæ­£çš„æ¨ç†åœ¨ `engine_core` ä¸­

2. **æ¨ç†è·¯å¾„**ï¼š
   ```
   llm_engine.step()
     â†’ engine_core.get_output()
       â†’ EngineCore.step()
         â†’ EngineCore._execute_model()
           â†’ GPUExecutor.execute_model()
             â†’ GPUWorker.execute_model()
               â†’ ModelRunner.execute_model()
                 â†’ Model.forward()
                   â†’ Layer.forward()
                     â†’ Attention/MLP/RMSNorm
                       â†’ CUDA Kernels
   ```

3. **å¦‚ä½•æ‰¾åˆ°ç®—å­è°ƒç”¨**ï¼Ÿ
   - åœ¨ `vllm/model_executor/layers/` ä¸­æ‰¾åˆ°å¯¹åº”çš„ Layer
   - çœ‹ `forward()` æ–¹æ³•
   - æ‰¾åˆ° `torch.ops.vllm.*` çš„è°ƒç”¨
   - è¿™äº›å°±æ˜¯ CUDA kernel çš„å…¥å£

### ä¸‹ä¸€æ­¥

å¦‚æœä½ æƒ³æ·»åŠ  SonicMoEï¼Œéœ€è¦ï¼š
1. åœ¨ `vllm/model_executor/layers/sonic_moe.py` åˆ›å»º Layer
2. åœ¨æ¨¡å‹ä¸­ä½¿ç”¨ï¼ˆå¦‚ `Qwen2DecoderLayer`ï¼‰
3. Layer å†…éƒ¨è°ƒç”¨ä½ çš„ CUDA kernel

è¿è¡Œæˆ‘çš„è¿½è¸ªè„šæœ¬ï¼Œä½ ä¼šçœ‹åˆ°å®Œæ•´çš„è°ƒç”¨è·¯å¾„ï¼ğŸ¯
