# EngineCore.step() æ‰§è¡Œå®Œåçš„å®Œæ•´æµç¨‹

## ğŸ¯ ä½ çš„é—®é¢˜

**é—®**ï¼š`EngineCore.step()` æ‰§è¡Œåˆ°è¿™é‡Œåå‘¢ï¼Ÿ

```python
engine_core_outputs = self.scheduler.update_from_output(
    scheduler_output, model_output
)
return engine_core_outputs, scheduler_output.total_num_scheduled_tokens > 0
```

**ç­”**ï¼šè¿”å›åˆ° `InprocClient.get_output()`ï¼Œç„¶åä¸€å±‚å±‚è¿”å›ç»™ç”¨æˆ·ä»£ç ã€‚

---

## ğŸ“Š å®Œæ•´çš„è¿”å›è·¯å¾„

```
EngineCore.step() è¿”å›
    â†“
    return (engine_core_outputs, model_executed)
    â†“
InprocClient.get_output() æ¥æ”¶
    â†“
    outputs, model_executed = self.engine_core.step_fn()
    â†“
    return outputs.get(0) or EngineCoreOutputs()
    â†“
LLMEngine.step() æ¥æ”¶
    â†“
    outputs = self.engine_core.get_output()
    â†“
    processed_outputs = self.output_processor.process_outputs(outputs)
    â†“
    return processed_outputs.request_outputs
    â†“
LLM.generate() æ¥æ”¶
    â†“
    while engine.has_unfinished_requests():
        outputs = engine.step()  â† å¾—åˆ°è¿™ä¸€æ­¥çš„è¾“å‡º
        for output in outputs:
            if output.finished:
                final_outputs.append(output)
    â†“
è¿”å›ç»™ç”¨æˆ·
    â†“
ä½ çš„ä»£ç 
    outputs = llm.generate(prompts)
    print(outputs[0].outputs[0].text)  â† æœ€ç»ˆç»“æœ
```

---

## ğŸ” è¯¦ç»†åˆ†ææ¯ä¸€æ­¥

### Step 1: EngineCore.step() è¿”å›

```python
# æ–‡ä»¶ï¼švllm/v1/engine/core.py

def step(self) -> tuple[dict[int, EngineCoreOutputs], bool]:
    # ... æ‰§è¡Œæ¨ç† ...
    
    # æœ€åä¸€è¡Œï¼šè¿”å›ç»“æœ
    return engine_core_outputs, model_executed
    #      â†‘                    â†‘
    #      |                    æ˜¯å¦æ‰§è¡Œäº†æ¨¡å‹ï¼ˆboolï¼‰
    #      |
    #      è¾“å‡ºå­—å…¸ï¼š{wave_id: EngineCoreOutputs}

# engine_core_outputs çš„ç»“æ„ï¼š
# {
#     0: EngineCoreOutputs(
#         outputs=[
#             EngineCoreOutput(
#                 request_id="req_123",
#                 new_token_ids=[123, 456],  # æ–°ç”Ÿæˆçš„ token
#                 finish_reason=None,         # å¦‚æœå®Œæˆäº†ä¼šæœ‰å€¼
#             ),
#             ...
#         ]
#     )
# }
```

---

### Step 2: InprocClient.get_output() æ¥æ”¶å¹¶å¤„ç†

```python
# æ–‡ä»¶ï¼švllm/v1/engine/core_client.py

def get_output(self) -> EngineCoreOutputs:
    logger.info("ğŸ”¸ [InprocClient.get_output] è°ƒç”¨ EngineCore.step_fn()")
    
    # æ¥æ”¶ EngineCore.step() çš„è¿”å›å€¼
    outputs, model_executed = self.engine_core.step_fn()
    #                         â†‘
    #                         self.engine_core.step_fn æŒ‡å‘ EngineCore.step
    
    logger.info("   â†’ model_executed: %s", model_executed)
    logger.info("   â†’ outputs ç±»å‹: %s", type(outputs))
    
    # åå¤„ç†
    self.engine_core.post_step(model_executed=model_executed)
    
    # æå– wave 0 çš„è¾“å‡ºï¼ˆå¤§å¤šæ•°æƒ…å†µåªæœ‰ä¸€ä¸ª waveï¼‰
    return outputs and outputs.get(0) or EngineCoreOutputs()
    #      â†‘
    #      å¦‚æœ outputs ä¸ä¸ºç©ºï¼Œè·å– wave 0 çš„è¾“å‡º
    #      å¦åˆ™è¿”å›ç©ºçš„ EngineCoreOutputs
```

**post_step() åšä»€ä¹ˆï¼Ÿ**

```python
def post_step(self, model_executed: bool) -> None:
    """åå¤„ç†æ­¥éª¤"""
    
    # å¦‚æœä½¿ç”¨ speculative decodingï¼Œæ›´æ–° draft token ids
    if not self.async_scheduling and self.use_spec_decode and model_executed:
        draft_token_ids = self.model_executor.take_draft_token_ids()
        if draft_token_ids is not None:
            self.scheduler.update_draft_token_ids(draft_token_ids)
    
    # å¯¹äºä½ çš„ç®€å•åœºæ™¯ï¼Œè¿™é‡ŒåŸºæœ¬ä»€ä¹ˆéƒ½ä¸åš
```

---

### Step 3: LLMEngine.step() æ¥æ”¶å¹¶è½¬æ¢

```python
# æ–‡ä»¶ï¼švllm/v1/engine/llm_engine.py

def step(self) -> list[RequestOutput | PoolingRequestOutput]:
    logger.info("="*80)
    logger.info("ğŸ”¹ [LLMEngine.step] å¼€å§‹æ–°çš„æ¨ç† step")
    logger.info("="*80)
    
    # Step 1: ä» EngineCore è·å–è¾“å‡º
    logger.info("ğŸ“¥ [Step 1] ä» EngineCore è·å–è¾“å‡º...")
    outputs = self.engine_core.get_output()
    #         â†‘
    #         InprocClient.get_output() è¿”å›çš„ç»“æœ
    
    logger.info("âœ… [Step 1] è·å–åˆ° outputs")
    logger.info("   â†’ outputs ç±»å‹: %s", type(outputs).__name__)
    
    # Step 2: å¤„ç†è¾“å‡º - è½¬æ¢ä¸ºç”¨æˆ·å‹å¥½çš„æ ¼å¼
    logger.info("\nğŸ“Š [Step 2] å¤„ç†è¾“å‡º...")
    logger.info("   â†’ è°ƒç”¨ output_processor.process_outputs()")
    
    processed_outputs = self.output_processor.process_outputs(
        outputs.outputs,  # EngineCoreOutput åˆ—è¡¨
        engine_core_timestamp=outputs.timestamp,
        iteration_stats=iteration_stats,
    )
    
    logger.info("âœ… [Step 2] è¾“å‡ºå¤„ç†å®Œæˆ")
    logger.info("   â†’ è¿”å›çš„ RequestOutput æ•°é‡: %d", 
               len(processed_outputs.request_outputs))
    
    # Step 3: ä¸­æ­¢å·²å®Œæˆçš„è¯·æ±‚
    logger.info("\nğŸ—‘ï¸  [Step 3] å¤„ç†ä¸­æ­¢è¯·æ±‚")
    self.engine_core.abort_requests(processed_outputs.reqs_to_abort)
    
    # Step 4: è¿”å›ç”¨æˆ·å¯è§çš„è¾“å‡º
    logger.info("\nâœ… è¿”å› RequestOutput åˆ—è¡¨")
    return processed_outputs.request_outputs
    #      â†‘
    #      è¿™æ˜¯ç”¨æˆ·å‹å¥½çš„æ ¼å¼
    #      list[RequestOutput]
```

**OutputProcessor.process_outputs() åšä»€ä¹ˆï¼Ÿ**

```python
# æ–‡ä»¶ï¼švllm/v1/engine/output_processor.py

def process_outputs(
    self,
    outputs: list[EngineCoreOutput],
    engine_core_timestamp: float,
    iteration_stats: IterationStats | None,
) -> ProcessedOutputs:
    """
    è½¬æ¢ EngineCoreOutput -> RequestOutput
    
    EngineCoreOutput (å†…éƒ¨æ ¼å¼):
        - request_id: str
        - new_token_ids: list[int]
        - finish_reason: FinishReason | None
    
    RequestOutput (ç”¨æˆ·æ ¼å¼):
        - request_id: str
        - prompt: str
        - prompt_token_ids: list[int]
        - outputs: list[CompletionOutput]
            - text: str  â† è§£ç åçš„æ–‡æœ¬ï¼
            - token_ids: list[int]
            - finish_reason: str | None
    """
    
    request_outputs = []
    
    for output in outputs:
        # è·å–è¯·æ±‚çŠ¶æ€
        request_state = self.request_states[output.request_id]
        
        # ç´¯ç§¯æ–°ç”Ÿæˆçš„ tokens
        request_state.token_ids.extend(output.new_token_ids)
        
        # ğŸ”¥ è§£ç  tokens ä¸ºæ–‡æœ¬
        text = self.tokenizer.decode(
            request_state.token_ids,
            skip_special_tokens=True
        )
        
        # åˆ›å»º RequestOutput
        request_output = RequestOutput(
            request_id=output.request_id,
            prompt=request_state.prompt_text,
            prompt_token_ids=request_state.prompt_token_ids,
            outputs=[
                CompletionOutput(
                    text=text,  â† è¿™æ˜¯ç”¨æˆ·çœ‹åˆ°çš„æ–‡æœ¬ï¼
                    token_ids=request_state.token_ids,
                    finish_reason=output.finish_reason,
                )
            ],
            finished=(output.finish_reason is not None),
        )
        
        request_outputs.append(request_output)
    
    return ProcessedOutputs(request_outputs=request_outputs)
```

---

### Step 4: LLM.generate() å¾ªç¯æ¥æ”¶

```python
# æ–‡ä»¶ï¼švllm/entrypoints/llm.py

def generate(
    self,
    prompts: list[str],
    sampling_params: SamplingParams,
) -> list[RequestOutput]:
    """ç”¨æˆ·è°ƒç”¨çš„ç”Ÿæˆæ–¹æ³•"""
    
    logger.info("ğŸš€ [LLM.generate] å¼€å§‹ç”Ÿæˆ")
    logger.info("   Prompt: %s", prompts[0])
    logger.info("   Max tokens: %d", sampling_params.max_tokens)
    
    # æ·»åŠ è¯·æ±‚åˆ°å¼•æ“
    for prompt in prompts:
        request_id = f"req_{uuid.uuid4()}"
        self.engine.add_request(
            request_id=request_id,
            prompt=prompt,
            params=sampling_params,
        )
    
    # ğŸ”¥ æ ¸å¿ƒå¾ªç¯ï¼šä¸æ–­è°ƒç”¨ engine.step() ç›´åˆ°æ‰€æœ‰è¯·æ±‚å®Œæˆ
    final_outputs = []
    step_count = 0
    
    logger.info("\nâš¡ å¼€å§‹ç”Ÿæˆå¾ªç¯")
    
    while self.engine.has_unfinished_requests():
        step_count += 1
        logger.info("\n--- Step %d ---", step_count)
        
        # æ‰§è¡Œä¸€æ­¥æ¨ç†
        outputs = self.engine.step()
        #         â†‘
        #         è¿™è¿”å› list[RequestOutput]
        #         åŒ…å«è¿™ä¸€æ­¥æ‰€æœ‰è¯·æ±‚çš„è¾“å‡º
        
        logger.info("   â†’ æœ¬æ­¥è¾“å‡ºæ•°é‡: %d", len(outputs))
        
        # å¤„ç†æ¯ä¸ªè¾“å‡º
        for output in outputs:
            logger.info("   â†’ Request %s:", output.request_id)
            logger.info("     â€¢ å½“å‰æ–‡æœ¬: %s", output.outputs[0].text)
            logger.info("     â€¢ å·²ç”Ÿæˆ tokens: %d", len(output.outputs[0].token_ids))
            logger.info("     â€¢ æ˜¯å¦å®Œæˆ: %s", output.finished)
            
            if output.finished:
                logger.info("     âœ… è¯·æ±‚å®Œæˆï¼")
                final_outputs.append(output)
    
    logger.info("\nâœ… [LLM.generate] æ‰€æœ‰è¯·æ±‚å®Œæˆ")
    logger.info("   æ€»æ­¥æ•°: %d", step_count)
    
    return final_outputs
```

---

### Step 5: è¿”å›ç»™ç”¨æˆ·

```python
# ä½ çš„ä»£ç 
outputs = llm.generate(prompts, sampling_params)
#         â†‘
#         è¿™é‡Œæ¥æ”¶åˆ° list[RequestOutput]

# è®¿é—®ç»“æœ
for output in outputs:
    print(f"Generated: {output.outputs[0].text}")
    #                   â†‘
    #                   è¿™æ˜¯è§£ç åçš„å®Œæ•´æ–‡æœ¬
```

---

## ğŸ”„ å•ä¸ª Token ç”Ÿæˆçš„å®Œæ•´æ•°æ®æµ

```
Step 1: ç”¨æˆ·è°ƒç”¨
    llm.generate(["Tell me a joke"])
    â†“

Step 2: æ·»åŠ è¯·æ±‚
    engine.add_request(request_id="req_123", prompt="Tell me a joke")
    â†“

Step 3: å¾ªç¯å¼€å§‹
    while engine.has_unfinished_requests():
    â†“

Step 4: æ‰§è¡Œæ¨ç†ï¼ˆç¬¬ä¸€æ¬¡ï¼‰
    outputs = engine.step()
        â†“
    LLMEngine.step()
        â†“
    engine_core.get_output()
        â†“
    InprocClient.get_output()
        â†“
    EngineCore.step()
        â”œâ”€ scheduler.schedule() â†’ è°ƒåº¦ prompt tokens
        â”œâ”€ model_executor.execute_model() â†’ å‰å‘ä¼ æ’­
        â”‚    â”œâ”€ Model.forward() â†’ Transformer è®¡ç®—
        â”‚    â””â”€ è¿”å› logits: [batch, vocab_size]
        â”œâ”€ sample_tokens() â†’ ä» logits é‡‡æ ·ä¸‹ä¸€ä¸ª token
        â”‚    â””â”€ å¾—åˆ° token_id = 1234
        â””â”€ scheduler.update_from_output() â†’ æ›´æ–°çŠ¶æ€
             â””â”€ è¿”å› EngineCoreOutput(
                    request_id="req_123",
                    new_token_ids=[1234],  â† æ–° token
                    finish_reason=None
                )
    â†“
    output_processor.process_outputs()
        â”œâ”€ ç´¯ç§¯ tokens: [1234]
        â”œâ”€ è§£ç : tokenizer.decode([1234]) = "Why"
        â””â”€ è¿”å› RequestOutput(
               text="Why",  â† å½“å‰æ–‡æœ¬
               finished=False
           )
    â†“
    è¿”å›ç»™ generate() å¾ªç¯
    â†“

Step 5: ç»§ç»­å¾ªç¯ï¼ˆç¬¬äºŒæ¬¡ï¼‰
    outputs = engine.step()
        ... åŒæ ·çš„æµç¨‹ ...
        æ–° token_id = 5678
        ç´¯ç§¯ tokens: [1234, 5678]
        è§£ç : "Why do"
        è¿”å› RequestOutput(text="Why do", ...)
    â†“

... é‡å¤å¤šæ¬¡ ...

Step N: æœ€åä¸€æ¬¡ï¼ˆé‡åˆ°åœæ­¢æ¡ä»¶ï¼‰
    outputs = engine.step()
        æ–° token_id = EOS_TOKEN
        finish_reason = FinishReason.STOP
        è¿”å› RequestOutput(
            text="Why do software engineers...",  â† å®Œæ•´æ–‡æœ¬
            finished=True  â† æ ‡è®°å®Œæˆ
        )
    â†“

Step N+1: é€€å‡ºå¾ªç¯
    has_unfinished_requests() â†’ False
    â†“
    è¿”å› final_outputs
    â†“

ç”¨æˆ·æ¥æ”¶ç»“æœ
    outputs = [RequestOutput(text="Why do software engineers...")]
```

---

## ğŸ“Š æ•°æ®ç±»å‹è½¬æ¢é“¾

```
EngineCore å†…éƒ¨æ ¼å¼ï¼š
EngineCoreOutput
â”œâ”€â”€ request_id: str
â”œâ”€â”€ new_token_ids: list[int]  â† åŸå§‹ token IDs
â””â”€â”€ finish_reason: FinishReason | None
    â†“
    OutputProcessor è½¬æ¢
    â†“
ç”¨æˆ·å¯è§æ ¼å¼ï¼š
RequestOutput
â”œâ”€â”€ request_id: str
â”œâ”€â”€ prompt: str
â”œâ”€â”€ prompt_token_ids: list[int]
â””â”€â”€ outputs: list[CompletionOutput]
    â””â”€â”€ CompletionOutput
        â”œâ”€â”€ text: str  â† è§£ç åçš„æ–‡æœ¬ï¼
        â”œâ”€â”€ token_ids: list[int]
        â””â”€â”€ finish_reason: str | None
```

---

## ğŸ¯ å…³é”®ç†è§£

### 1. è¿”å›è·¯å¾„æ˜¯é€å±‚è¿”å›çš„

```python
EngineCore.step()
    return engine_core_outputs, model_executed
    â†“
InprocClient.get_output()
    outputs, model_executed = self.engine_core.step_fn()
    return outputs.get(0)
    â†“
LLMEngine.step()
    outputs = self.engine_core.get_output()
    processed = self.output_processor.process_outputs(outputs)
    return processed.request_outputs
    â†“
LLM.generate()
    outputs = self.engine.step()
    # æ”¶é›†æ‰€æœ‰å®Œæˆçš„è¾“å‡º
    â†“
ä½ çš„ä»£ç 
    outputs = llm.generate(...)
```

### 2. æ¯ä¸€æ­¥éƒ½åœ¨è½¬æ¢æ•°æ®æ ¼å¼

```
EngineCore.step()
    â†“ EngineCoreOutput (å†…éƒ¨æ ¼å¼ï¼ŒåŒ…å« token IDs)
InprocClient.get_output()
    â†“ EngineCoreOutputs (åŒ…è£…æ ¼å¼)
LLMEngine.step()
    â†“ OutputProcessor è½¬æ¢
    â†“ RequestOutput (ç”¨æˆ·æ ¼å¼ï¼ŒåŒ…å«è§£ç åçš„æ–‡æœ¬)
LLM.generate()
    â†“ list[RequestOutput]
ä½ çš„ä»£ç 
    â†“ æœ€ç»ˆç»“æœ
```

### 3. æ˜¯ä¸€ä¸ªå¾ªç¯è¿‡ç¨‹

```python
while engine.has_unfinished_requests():
    # æ¯æ¬¡å¾ªç¯ç”Ÿæˆä¸€ä¸ªæˆ–å¤šä¸ª token
    outputs = engine.step()
    
    # å¦‚æœè¯·æ±‚å®Œæˆäº†ï¼Œæ”¶é›†ç»“æœ
    for output in outputs:
        if output.finished:
            final_outputs.append(output)

# æ‰€æœ‰è¯·æ±‚å®Œæˆåé€€å‡ºå¾ªç¯
return final_outputs
```

---

## ğŸ“ æ€»ç»“

**`EngineCore.step()` æ‰§è¡Œå®Œåçš„æµç¨‹**ï¼š

1. **è¿”å›ç»“æœ** â†’ `InprocClient.get_output()`
2. **æå–è¾“å‡º** â†’ è·å– wave 0 çš„ `EngineCoreOutputs`
3. **åå¤„ç†** â†’ `post_step()` åšæ¸…ç†å·¥ä½œ
4. **è¿”å›** â†’ `LLMEngine.step()`
5. **è½¬æ¢æ ¼å¼** â†’ `OutputProcessor` è§£ç  tokens ä¸ºæ–‡æœ¬
6. **è¿”å›** â†’ `LLM.generate()` å¾ªç¯
7. **æ£€æŸ¥æ˜¯å¦å®Œæˆ** â†’ å¦‚æœå®Œæˆï¼Œé€€å‡ºå¾ªç¯
8. **è¿”å›ç»™ç”¨æˆ·** â†’ æœ€ç»ˆçš„ `list[RequestOutput]`

**å…³é”®ç‚¹**ï¼š
- âœ… æ¯æ¬¡ `step()` ç”Ÿæˆ 1 ä¸ªæˆ–å¤šä¸ª tokens
- âœ… ç»“æœé€å±‚è¿”å›ï¼Œæ¯å±‚åšä¸åŒçš„å¤„ç†
- âœ… OutputProcessor è´Ÿè´£è§£ç  tokens ä¸ºæ–‡æœ¬
- âœ… generate() å¾ªç¯ç›´åˆ°æ‰€æœ‰è¯·æ±‚å®Œæˆ

ç°åœ¨ä½ æ¸…æ¥š `step()` æ‰§è¡Œå®Œåçš„å®Œæ•´æµç¨‹äº†ï¼ğŸ¯
