# vLLM æ¨ç†è·¯å¾„è¿½è¸ªå·¥å…·

è¿™äº›è„šæœ¬å¸®åŠ©ä½ è¿½è¸ª vLLM æ¨ç†è¿‡ç¨‹ä¸­çš„ç®—å­è°ƒç”¨å’Œ CUDA kernel æ‰§è¡Œã€‚

## ğŸ“ æ–‡ä»¶è¯´æ˜

### 1. `test_simple_trace.py` - ç®€å•è¿½è¸ªï¼ˆæ¨èï¼‰

**åŠŸèƒ½**ï¼š
- è¿½è¸ªæ¯ä¸ª Layer çš„è°ƒç”¨
- ç»Ÿè®¡æ¨¡å—è°ƒç”¨æ¬¡æ•°
- æ˜¾ç¤ºæ‰§è¡Œè·¯å¾„

**ä½¿ç”¨**ï¼š
```bash
python test_simple_trace.py
```

**è¾“å‡ºç¤ºä¾‹**ï¼š
```
ğŸ“‹ æ¨¡å—è°ƒç”¨æ¬¡æ•°:
  â€¢ RMSNorm: 48 æ¬¡
  â€¢ Attention: 24 æ¬¡
  â€¢ MLP: 24 æ¬¡
  â€¢ QKVParallelLinear: 24 æ¬¡
  ...

ç¬¬ä¸€å±‚ Transformer çš„è°ƒç”¨é¡ºåº:
 1. [RMSNorm] input_layernorm
     Input:  (1, 7, 1536)
     Output: (1, 7, 1536)
 2. [QKVParallelLinear] qkv_proj
     Input:  (1, 7, 1536)
     Output: (1, 7, 4608)
 ...
```

---

### 2. `test_inference_with_logs.py` - è¯¦ç»†è¿½è¸ª

**åŠŸèƒ½**ï¼š
- æ›´è¯¦ç»†çš„æ¨¡å—ä¿¡æ¯
- è°ƒç”¨æ ˆè¿½è¸ª
- æŒ‰æ¨¡å—ç±»å‹åˆ†ç»„ç»Ÿè®¡

**ä½¿ç”¨**ï¼š
```bash
python test_inference_with_logs.py
```

**è¾“å‡ºç¤ºä¾‹**ï¼š
```
ğŸ”¸ [RMSNorm] model.layers.0.input_layernorm
   â”œâ”€ Input: (1, 7, 1536)
   â””â”€ Output: (1, 7, 1536)

ğŸ”¸ [Qwen2Attention] model.layers.0.self_attn
   â”œâ”€ Input: (1, 7, 1536)
   â””â”€ Output: (1, 7, 1536)
   
Top 10 æœ€é¢‘ç¹è°ƒç”¨çš„æ¨¡å—:
  1. RMSNorm:model.layers.0.input_layernorm: 2 æ¬¡
  2. Attention:model.layers.0.self_attn: 2 æ¬¡
  ...
```

---

### 3. `test_cuda_profiler.py` - CUDA Kernel è¿½è¸ªï¼ˆæœ€è¯¦ç»†ï¼‰

**åŠŸèƒ½**ï¼š
- ä½¿ç”¨ PyTorch Profiler
- è¿½è¸ªæ¯ä¸ª CUDA kernel çš„è°ƒç”¨
- æ˜¾ç¤º kernel è€—æ—¶å’Œè°ƒç”¨æ¬¡æ•°
- è¯†åˆ« vLLM æ ¸å¿ƒ kernels

**ä½¿ç”¨**ï¼š
```bash
python test_cuda_profiler.py
```

**è¾“å‡ºç¤ºä¾‹**ï¼š
```
ğŸ”¥ CUDA Kernel è°ƒç”¨ç»Ÿè®¡ï¼ˆTop 30ï¼‰
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

 1. paged_attention_v2_kernel
     Total:    15.234 ms | Calls:   24 | Avg:  0.635 ms
     
 2. fused_add_rms_norm_kernel
     Total:     8.456 ms | Calls:   48 | Avg:  0.176 ms
     
 3. rotary_embedding_kernel
     Total:     3.123 ms | Calls:   24 | Avg:  0.130 ms
     
 4. silu_and_mul_kernel
     Total:     2.789 ms | Calls:   24 | Avg:  0.116 ms
     
 ...

ğŸ¯ vLLM æ ¸å¿ƒ Kernels
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

1. PagedAttention Kernels:
   â€¢ paged_attention_v2_kernel
     Calls: 24, Total: 15.234 ms
   â€¢ paged_attention_v2_reduce_kernel
     Calls: 24, Total: 2.123 ms

2. RMSNorm Kernels:
   â€¢ fused_add_rms_norm_kernel
     Calls: 48, Total: 8.456 ms

3. Rotary Embedding Kernels:
   â€¢ rotary_embedding_kernel
     Calls: 24, Total: 3.123 ms

4. SiLU Activation Kernels:
   â€¢ silu_and_mul_kernel
     Calls: 24, Total: 2.789 ms
```

**é¢å¤–è¾“å‡º**ï¼š
- ç”Ÿæˆ `profiler_report.txt` - å®Œæ•´çš„ profiler æŠ¥å‘Šï¼ˆ100+ kernelsï¼‰

---

## ğŸ¯ ä½¿ç”¨åœºæ™¯

### åœºæ™¯ 1ï¼šå¿«é€Ÿäº†è§£æ‰§è¡Œè·¯å¾„
```bash
python test_simple_trace.py
```
â†’ çœ‹åˆ°æ¯ä¸ª Layer çš„è°ƒç”¨é¡ºåº

### åœºæ™¯ 2ï¼šè°ƒè¯•ç‰¹å®šæ¨¡å—
```bash
python test_inference_with_logs.py
```
â†’ è¯¦ç»†çš„è¾“å…¥è¾“å‡ºå½¢çŠ¶ï¼Œæ–¹ä¾¿è°ƒè¯•

### åœºæ™¯ 3ï¼šæ€§èƒ½åˆ†æ
```bash
python test_cuda_profiler.py
```
â†’ æ‰¾å‡ºæ€§èƒ½ç“¶é¢ˆçš„ CUDA kernel

---

## ğŸ“Š ç†è§£è¾“å‡º

### æ¯ä¸ª Token ç”Ÿæˆçš„ç®—å­è°ƒç”¨é¡ºåº

```
å¯¹äº 24 å±‚çš„æ¨¡å‹ï¼Œæ¯ç”Ÿæˆ 1 ä¸ª tokenï¼š

å¾ªç¯ 24 æ¬¡ï¼ˆæ¯å±‚ï¼‰ï¼š
  1. RMSNorm (Pre-Attention)     â†’ CUDA: rms_norm_kernel
  2. QKV Projection               â†’ cuBLAS: gemm
  3. Rotary Embedding             â†’ CUDA: rotary_embedding_kernel
  4. PagedAttention               â†’ CUDA: paged_attention_v2_kernel
                                         paged_attention_v2_reduce_kernel
  5. Output Projection            â†’ cuBLAS: gemm
  6. RMSNorm (Post-Attention)     â†’ CUDA: rms_norm_kernel
  7. Gate+Up Projection (MLP)     â†’ cuBLAS: gemm
  8. SiLU Activation              â†’ CUDA: silu_and_mul_kernel
  9. Down Projection (MLP)        â†’ cuBLAS: gemm

æ€»è®¡æ¯ä¸ª tokenï¼š
- RMSNorm: 48 æ¬¡ï¼ˆ24 å±‚ Ã— 2ï¼‰
- PagedAttention: 24 æ¬¡ï¼ˆ24 å±‚ Ã— 1ï¼‰
- Rotary Embedding: 24 æ¬¡
- SiLU: 24 æ¬¡
- Linear (cuBLAS): 96 æ¬¡ï¼ˆ24 å±‚ Ã— 4ï¼‰
```

### CUDA Kernel åˆ°æºç çš„æ˜ å°„

| Kernel åç§° | æºç ä½ç½® |
|------------|---------|
| `paged_attention_v2_kernel` | `csrc/attention/paged_attention_v2.cu` |
| `paged_attention_v2_reduce_kernel` | `csrc/attention/paged_attention_v2.cu` |
| `fused_add_rms_norm_kernel` | `csrc/ops/layernorm.cu` |
| `rotary_embedding_kernel` | `csrc/ops/rotary_embedding.cu` |
| `silu_and_mul_kernel` | `csrc/ops/activation.cu` |

---

## ğŸ”§ è‡ªå®šä¹‰è¿½è¸ª

### è¿½è¸ªç‰¹å®šæ¨¡å—

ä¿®æ”¹ `test_simple_trace.py`ï¼š

```python
# åœ¨ track_layer_calls å‡½æ•°ä¸­ä¿®æ”¹è¿‡æ»¤æ¡ä»¶

# åªè¿½è¸ª Attention
if 'Attention' in module_type:
    hook = module.register_forward_hook(...)

# åªè¿½è¸ª MLP
if 'MLP' in module_type:
    hook = module.register_forward_hook(...)

# è¿½è¸ªä½ çš„è‡ªå®šä¹‰æ¨¡å—
if 'SonicMoE' in module_type:
    hook = module.register_forward_hook(...)
```

### ä¿®æ”¹ç”Ÿæˆå‚æ•°

```python
# ç”Ÿæˆæ›´å¤š tokens
sampling_params = SamplingParams(
    temperature=0.8,
    top_p=0.95,
    max_tokens=100,  # æ”¹è¿™é‡Œ
)

# ä½¿ç”¨ä¸åŒçš„ prompt
prompts = ["Your custom prompt here"]
```

---

## ğŸ’¡ å¸¸è§é—®é¢˜

### Q1: ä¸ºä»€ä¹ˆçœ‹ä¸åˆ°å…·ä½“çš„ CUDA kernel åç§°ï¼Ÿ

A: ä½¿ç”¨ `test_cuda_profiler.py`ï¼Œå®ƒä¼šæ˜¾ç¤ºæ‰€æœ‰ CUDA kernelã€‚

### Q2: å¦‚ä½•è¿½è¸ªæˆ‘æ·»åŠ çš„æ–°ç®—å­ï¼ˆå¦‚ SonicMoEï¼‰ï¼Ÿ

A: ä¿®æ”¹è¿½è¸ªè„šæœ¬ä¸­çš„ `key_modules` åˆ—è¡¨ï¼š

```python
key_modules = [
    'RMSNorm', 'Attention', 'MLP', 
    'SonicMoE',  # â† æ·»åŠ ä½ çš„æ¨¡å—
]
```

### Q3: è¾“å‡ºå¤ªå¤šï¼Œå¦‚ä½•è¿‡æ»¤ï¼Ÿ

A: ä¿®æ”¹è„šæœ¬ä¸­çš„æ˜¾ç¤ºæ•°é‡ï¼š

```python
# åªæ˜¾ç¤ºå‰ 10 ä¸ª
for i, entry in enumerate(call_log[:10], 1):
    ...

# åªæ˜¾ç¤ºç‰¹å®šå±‚
first_layer_calls = [
    entry for entry in call_log 
    if 'layers.0.' in entry['name']  # åªçœ‹ç¬¬ 0 å±‚
]
```

### Q4: å¦‚ä½•ä¿å­˜è¿½è¸ªç»“æœï¼Ÿ

A: åœ¨è„šæœ¬æœ«å°¾æ·»åŠ ï¼š

```python
# ä¿å­˜åˆ°æ–‡ä»¶
with open('trace_result.txt', 'w') as f:
    for entry in call_log:
        f.write(f"{entry}\n")
```

---

## ğŸ“ å­¦ä¹ å»ºè®®

1. **ç¬¬ä¸€æ¬¡è¿è¡Œ**ï¼šä½¿ç”¨ `test_simple_trace.py`
   - ç†è§£åŸºæœ¬çš„æ‰§è¡Œæµç¨‹
   - çœ‹åˆ°æ¯ä¸ªæ¨¡å—çš„è°ƒç”¨æ¬¡æ•°

2. **æ·±å…¥ç†è§£**ï¼šä½¿ç”¨ `test_cuda_profiler.py`
   - çœ‹åˆ°å®é™…çš„ CUDA kernel è°ƒç”¨
   - ç†è§£å“ªäº›æ“ä½œæœ€è€—æ—¶

3. **é›†æˆæ–°ç®—å­**ï¼šå‚è€ƒè¿™äº›è„šæœ¬
   - ç¡®è®¤ä½ çš„ç®—å­è¢«æ­£ç¡®è°ƒç”¨
   - å¯¹æ¯”æ€§èƒ½ï¼ˆè°ƒç”¨æ¬¡æ•°ã€è€—æ—¶ï¼‰

---

## ğŸ“š ç›¸å…³æ–‡æ¡£

- [vLLM æ¶æ„æ–‡æ¡£](../docs/paged_attention_v2_analysis.md)
- [CoW ä»£ç ä½ç½®æŒ‡å—](../docs/cow_code_locations.md)

---

**æç¤º**ï¼šè¿™äº›è„šæœ¬ä¼šåœ¨æ¨ç†æ—¶æ·»åŠ  hookï¼Œå¯èƒ½ä¼šç•¥å¾®å½±å“æ€§èƒ½ã€‚ç”Ÿäº§ç¯å¢ƒè¯·ç§»é™¤è¿½è¸ªä»£ç ã€‚
